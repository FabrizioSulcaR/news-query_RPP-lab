{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RPP News Retrieval and Embedding System\n",
        "\n",
        "## Task 1 ‚Äî News Retrieval and Embedding System (RPP RSS Feed)\n",
        "\n",
        "This notebook demonstrates an end-to-end news retrieval system that:\n",
        "1. Ingests news from RPP RSS feed\n",
        "2. Tokenizes and analyzes text\n",
        "3. Generates embeddings using SentenceTransformers\n",
        "4. Stores documents in ChromaDB\n",
        "5. Performs semantic similarity search\n",
        "6. Orchestrates everything with LangChain\n",
        "\n",
        "**Note**: All code is self-contained in this notebook - no external modules required.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run if you're using Google Colab\n",
        "\n",
        "#!pip install -U \\\n",
        "#  \"feedparser>=6.0.11\" \"tiktoken>=0.5.2\" \"sentence-transformers>=2.2.2\" \\\n",
        "#  \"chromadb>=0.4.22\" \"langchain>=0.1.0\" \"langchain-community>=0.0.10\" \\\n",
        "#  \"pandas>=2.0.3\" \"jupyter>=1.0.0\" \"notebook>=7.0.0\" \"numpy>=1.24.0\" \\\n",
        "#  \"requests==2.32.4\"\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# Third-party imports\n",
        "import feedparser\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. RSS Feed Ingestion\n",
        "\n",
        "### Step 0Ô∏è‚É£: Load Data from RPP RSS Feed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching RSS feed from: https://rpp.pe/rss\n",
            "Successfully fetched 50 articles\n",
            "Articles saved to: ../data/rss_feed.json\n",
            "\n",
            "üìä Total articles fetched: 50\n",
            "\n",
            "üì∞ Sample article:\n",
            "Title: ¬øPor qu√© es importante el consumo de agua para cuidar tus ri√±ones?\n",
            "Description: El doctor Mario Encinas, jefe de Nefrolog√≠a del Instituto Nacional de Salud del Ni√±o de Bre√±a, abord...\n",
            "Link: https://rpp.pe/vital/salud/cuidar-tus-rinones-importancia-del-consumo-de-agua-noticia-1660660\n",
            "Published: Thu, 23 Oct 2025 13:38:26 -0500\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "# RSS Parser Functions\n",
        "def fetch_rpp_news(rss_url: str = \"https://rpp.pe/rss\", max_items: int = 50) -> List[Dict]:\n",
        "    \"\"\"Fetch and parse RPP RSS feed\"\"\"\n",
        "    print(f\"Fetching RSS feed from: {rss_url}\")\n",
        "    r = requests.get(rss_url, timeout=15)\n",
        "    feed = feedparser.parse(r.content)\n",
        "    \n",
        "    articles = []\n",
        "    for entry in feed.entries[:max_items]:\n",
        "        article = {\n",
        "            \"title\": entry.get(\"title\", \"\"),\n",
        "            \"description\": entry.get(\"description\", \"\"),\n",
        "            \"link\": entry.get(\"link\", \"\"),\n",
        "            \"published\": entry.get(\"published\", \"\")\n",
        "        }\n",
        "        articles.append(article)\n",
        "    \n",
        "    print(f\"Successfully fetched {len(articles)} articles\")\n",
        "    return articles\n",
        "\n",
        "def save_articles_to_json(articles: List[Dict], output_path: str = \"../data/rss_feed.json\"):\n",
        "    \"\"\"Save articles to JSON file\"\"\"\n",
        "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Articles saved to: {output_path}\")\n",
        "\n",
        "# Fetch 50 latest articles from RPP RSS feed\n",
        "RSS_URL = \"https://rpp.pe/rss\"\n",
        "MAX_ARTICLES = 50\n",
        "\n",
        "articles = fetch_rpp_news(rss_url=RSS_URL, max_items=MAX_ARTICLES)\n",
        "save_articles_to_json(articles)\n",
        "\n",
        "print(f\"\\nüìä Total articles fetched: {len(articles)}\")\n",
        "print(f\"\\nüì∞ Sample article:\")\n",
        "print(f\"Title: {articles[0]['title']}\")\n",
        "print(f\"Description: {articles[0]['description'][:100]}...\")\n",
        "print(f\"Link: {articles[0]['link']}\")\n",
        "print(f\"Published: {articles[0]['published']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã First 5 articles:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>¬øPor qu√© es importante el consumo de agua para...</td>\n",
              "      <td>Thu, 23 Oct 2025 13:38:26 -0500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Alianza Lima anunci√≥ la renovaci√≥n de N√©stor G...</td>\n",
              "      <td>Thu, 23 Oct 2025 13:08:02 -0500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lula da Silva confirma que se presentar√° para ...</td>\n",
              "      <td>Thu, 23 Oct 2025 07:47:19 -0500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jefe de la Dirincri informa que hay c√°maras de...</td>\n",
              "      <td>Thu, 23 Oct 2025 13:05:36 -0500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Un sue√±o hecho realidad: Iron Maiden anuncia n...</td>\n",
              "      <td>Thu, 23 Oct 2025 13:13:55 -0500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  ¬øPor qu√© es importante el consumo de agua para...   \n",
              "1  Alianza Lima anunci√≥ la renovaci√≥n de N√©stor G...   \n",
              "2  Lula da Silva confirma que se presentar√° para ...   \n",
              "3  Jefe de la Dirincri informa que hay c√°maras de...   \n",
              "4  Un sue√±o hecho realidad: Iron Maiden anuncia n...   \n",
              "\n",
              "                         published  \n",
              "0  Thu, 23 Oct 2025 13:38:26 -0500  \n",
              "1  Thu, 23 Oct 2025 13:08:02 -0500  \n",
              "2  Thu, 23 Oct 2025 07:47:19 -0500  \n",
              "3  Thu, 23 Oct 2025 13:05:36 -0500  \n",
              "4  Thu, 23 Oct 2025 13:13:55 -0500  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display first 5 articles as DataFrame\n",
        "df_articles = pd.DataFrame(articles)\n",
        "print(\"\\nüìã First 5 articles:\")\n",
        "df_articles[['title', 'published']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenization Analysis\n",
        "\n",
        "### Step 1Ô∏è‚É£: Tokenization using tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî§ Token Analysis for Sample Article:\n",
            "Title: ¬øPor qu√© es importante el consumo de agua para cuidar tus ri√±ones?...\n",
            "\n",
            "Title tokens: 17\n",
            "Description tokens: 58\n",
            "Total tokens: 75\n",
            "\n",
            "‚ö†Ô∏è  Needs chunking (512 token limit): False\n",
            "Token count: 75\n"
          ]
        }
      ],
      "source": [
        "# Tokenization Functions\n",
        "def count_tokens(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
        "    \"\"\"Count tokens in text using tiktoken\"\"\"\n",
        "    encoder = tiktoken.get_encoding(encoding_name)\n",
        "    tokens = encoder.encode(text)\n",
        "    return len(tokens)\n",
        "\n",
        "def analyze_article_tokens(article: Dict, encoding_name: str = \"cl100k_base\") -> Dict:\n",
        "    \"\"\"Analyze token counts for an article\"\"\"\n",
        "    title = article.get(\"title\", \"\")\n",
        "    description = article.get(\"description\", \"\")\n",
        "    full_text = f\"{title}\\n{description}\"\n",
        "    \n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"title_tokens\": count_tokens(title, encoding_name),\n",
        "        \"description_tokens\": count_tokens(description, encoding_name),\n",
        "        \"total_tokens\": count_tokens(full_text, encoding_name),\n",
        "        \"full_text\": full_text\n",
        "    }\n",
        "\n",
        "def needs_chunking(text: str, max_tokens: int = 512, encoding_name: str = \"cl100k_base\") -> Tuple[bool, int]:\n",
        "    \"\"\"Determine if text needs chunking\"\"\"\n",
        "    token_count = count_tokens(text, encoding_name)\n",
        "    return token_count > max_tokens, token_count\n",
        "\n",
        "# Analyze a sample article\n",
        "sample_article = articles[0]\n",
        "token_analysis = analyze_article_tokens(sample_article)\n",
        "\n",
        "print(\"üî§ Token Analysis for Sample Article:\")\n",
        "print(f\"Title: {token_analysis['title'][:80]}...\")\n",
        "print(f\"\\nTitle tokens: {token_analysis['title_tokens']}\")\n",
        "print(f\"Description tokens: {token_analysis['description_tokens']}\")\n",
        "print(f\"Total tokens: {token_analysis['total_tokens']}\")\n",
        "\n",
        "needs_chunk, token_count = needs_chunking(token_analysis['full_text'], max_tokens=512)\n",
        "print(f\"\\n‚ö†Ô∏è  Needs chunking (512 token limit): {needs_chunk}\")\n",
        "print(f\"Token count: {token_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Corpus Token Statistics:\n",
            "Number of articles: 50\n",
            "Total tokens: 3,883\n",
            "Average tokens per article: 77.66\n",
            "Min tokens: 40\n",
            "Max tokens: 138\n",
            "\n",
            "üìà Articles exceeding 512 tokens: 0/50\n",
            "Percentage: 0.00%\n"
          ]
        }
      ],
      "source": [
        "def analyze_corpus_tokens(articles: List[Dict], encoding_name: str = \"cl100k_base\") -> Dict:\n",
        "    \"\"\"Analyze token statistics for entire corpus\"\"\"\n",
        "    token_counts = [analyze_article_tokens(article, encoding_name)[\"total_tokens\"] \n",
        "                    for article in articles]\n",
        "    \n",
        "    return {\n",
        "        \"num_articles\": len(articles),\n",
        "        \"total_tokens\": sum(token_counts),\n",
        "        \"avg_tokens\": sum(token_counts) / len(token_counts) if token_counts else 0,\n",
        "        \"min_tokens\": min(token_counts) if token_counts else 0,\n",
        "        \"max_tokens\": max(token_counts) if token_counts else 0,\n",
        "        \"token_counts\": token_counts\n",
        "    }\n",
        "\n",
        "# Analyze entire corpus\n",
        "corpus_stats = analyze_corpus_tokens(articles)\n",
        "\n",
        "print(\"\\nüìä Corpus Token Statistics:\")\n",
        "print(f\"Number of articles: {corpus_stats['num_articles']}\")\n",
        "print(f\"Total tokens: {corpus_stats['total_tokens']:,}\")\n",
        "print(f\"Average tokens per article: {corpus_stats['avg_tokens']:.2f}\")\n",
        "print(f\"Min tokens: {corpus_stats['min_tokens']}\")\n",
        "print(f\"Max tokens: {corpus_stats['max_tokens']}\")\n",
        "\n",
        "articles_needing_chunking = sum(1 for count in corpus_stats['token_counts'] if count > 512)\n",
        "print(f\"\\nüìà Articles exceeding 512 tokens: {articles_needing_chunking}/{corpus_stats['num_articles']}\")\n",
        "print(f\"Percentage: {(articles_needing_chunking/corpus_stats['num_articles'])*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Embedding Generation\n",
        "\n",
        "### Step 2Ô∏è‚É£: Generate embeddings using SentenceTransformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08520bffcbd34231a3bdcd9061d918ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  \u001b[2m2025-10-23T18:53:02.176906Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x105b84770>), traceback: Some(<traceback object at 0x1625c5c80>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
            "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
            "\n",
            "  \u001b[2m2025-10-23T18:53:02.177146Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x105b84770>), traceback: Some(<traceback object at 0x1625c5e00>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
            "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
            "\n",
            "  \u001b[2m2025-10-23T18:53:02.178616Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x105b84770>), traceback: Some(<traceback object at 0x1625c6040>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
            "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# News Embedder Class\n",
        "class NewsEmbedder:\n",
        "    \"\"\"Wrapper class for generating news embeddings\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        print(f\"Loading embedding model: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.model_name = model_name\n",
        "        print(f\"Model loaded. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "    \n",
        "    def embed_text(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for a single text\"\"\"\n",
        "        return self.model.encode(text, convert_to_numpy=True)\n",
        "    \n",
        "    def embed_articles(self, articles: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Generate embeddings for multiple articles\"\"\"\n",
        "        print(f\"Generating embeddings for {len(articles)} articles...\")\n",
        "        \n",
        "        texts = [f\"{article.get('title', '')}\\n{article.get('description', '')}\" \n",
        "                 for article in articles]\n",
        "        \n",
        "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "        \n",
        "        embedded_articles = []\n",
        "        for article, embedding in zip(articles, embeddings):\n",
        "            embedded_article = article.copy()\n",
        "            embedded_article[\"embedding\"] = embedding\n",
        "            embedded_article[\"text\"] = f\"{article.get('title', '')}\\n{article.get('description', '')}\"\n",
        "            embedded_articles.append(embedded_article)\n",
        "        \n",
        "        print(f\"Embeddings generated. Shape: {embeddings.shape}\")\n",
        "        return embedded_articles\n",
        "    \n",
        "    def get_embedding_dimension(self) -> int:\n",
        "        return self.model.get_sentence_embedding_dimension()\n",
        "\n",
        "# Initialize embedder\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = NewsEmbedder(model_name=MODEL_NAME)\n",
        "\n",
        "print(f\"\\n‚úÖ Embedder initialized\")\n",
        "print(f\"Model: {embedder.model_name}\")\n",
        "print(f\"Embedding dimension: {embedder.get_embedding_dimension()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for all articles\n",
        "embedded_articles = embedder.embed_articles(articles)\n",
        "\n",
        "print(f\"\\n‚úÖ Embeddings generated for {len(embedded_articles)} articles\")\n",
        "print(f\"\\nSample embedding:\")\n",
        "print(f\"Shape: {embedded_articles[0]['embedding'].shape}\")\n",
        "print(f\"First 10 values: {embedded_articles[0]['embedding'][:10]}\")\n",
        "print(f\"\\nEmbedding statistics:\")\n",
        "print(f\"Mean: {np.mean(embedded_articles[0]['embedding']):.4f}\")\n",
        "print(f\"Std: {np.std(embedded_articles[0]['embedding']):.4f}\")\n",
        "print(f\"Min: {np.min(embedded_articles[0]['embedding']):.4f}\")\n",
        "print(f\"Max: {np.max(embedded_articles[0]['embedding']):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ChromaDB Storage\n",
        "\n",
        "### Step 3Ô∏è‚É£: Create or Upsert Chroma Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChromaDB Retrieval Class\n",
        "class NewsRetriever:\n",
        "    \"\"\"Wrapper class for ChromaDB retrieval operations\"\"\"\n",
        "    \n",
        "    def __init__(self, collection_name: str = \"rpp_news\", persist_directory: str = \"../data/chromadb\"):\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_directory = persist_directory\n",
        "        Path(persist_directory).mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        print(f\"Initializing ChromaDB in: {persist_directory}\")\n",
        "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
        "        \n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=collection_name)\n",
        "            print(f\"Loaded existing collection: {collection_name}\")\n",
        "        except:\n",
        "            self.collection = self.client.create_collection(\n",
        "                name=collection_name,\n",
        "                metadata={\"hnsw:space\": \"cosine\"}\n",
        "            )\n",
        "            print(f\"Created new collection: {collection_name}\")\n",
        "    \n",
        "    def add_documents(self, articles: List[Dict]):\n",
        "        \"\"\"Add or upsert documents to the collection\"\"\"\n",
        "        print(f\"Adding {len(articles)} documents to collection...\")\n",
        "        \n",
        "        documents = []\n",
        "        embeddings = []\n",
        "        metadatas = []\n",
        "        ids = []\n",
        "        \n",
        "        for idx, article in enumerate(articles):\n",
        "            text = article.get(\"text\", f\"{article.get('title', '')}\\n{article.get('description', '')}\")\n",
        "            documents.append(text)\n",
        "            embeddings.append(article[\"embedding\"].tolist())\n",
        "            \n",
        "            metadata = {\n",
        "                \"title\": article.get(\"title\", \"\"),\n",
        "                \"description\": article.get(\"description\", \"\"),\n",
        "                \"link\": article.get(\"link\", \"\"),\n",
        "                \"published\": article.get(\"published\", \"\")\n",
        "            }\n",
        "            metadatas.append(metadata)\n",
        "            ids.append(f\"article_{idx}\")\n",
        "        \n",
        "        self.collection.upsert(\n",
        "            documents=documents,\n",
        "            embeddings=embeddings,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "        \n",
        "        print(f\"Successfully added {len(articles)} documents\")\n",
        "        print(f\"Total documents in collection: {self.collection.count()}\")\n",
        "    \n",
        "    def query(self, query_text: str, n_results: int = 5, embedder=None) -> Dict:\n",
        "        \"\"\"Query the collection with similarity search\"\"\"\n",
        "        print(f\"\\nQuerying: '{query_text}'\")\n",
        "        \n",
        "        if embedder:\n",
        "            query_embedding = embedder.embed_text(query_text).tolist()\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=[query_embedding],\n",
        "                n_results=n_results\n",
        "            )\n",
        "        else:\n",
        "            results = self.collection.query(\n",
        "                query_texts=[query_text],\n",
        "                n_results=n_results\n",
        "            )\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def query_to_dataframe(self, query_text: str, n_results: int = 5, embedder=None) -> pd.DataFrame:\n",
        "        \"\"\"Query and return results as pandas DataFrame\"\"\"\n",
        "        results = self.query(query_text, n_results, embedder)\n",
        "        \n",
        "        data = []\n",
        "        if results[\"metadatas\"] and len(results[\"metadatas\"]) > 0:\n",
        "            for metadata in results[\"metadatas\"][0]:\n",
        "                row = {\n",
        "                    \"title\": metadata.get(\"title\", \"\"),\n",
        "                    \"description\": metadata.get(\"description\", \"\"),\n",
        "                    \"link\": metadata.get(\"link\", \"\"),\n",
        "                    \"date_published\": metadata.get(\"published\", \"\")\n",
        "                }\n",
        "                data.append(row)\n",
        "        \n",
        "        return pd.DataFrame(data)\n",
        "    \n",
        "    def get_collection_stats(self) -> Dict:\n",
        "        \"\"\"Get statistics about the collection\"\"\"\n",
        "        return {\n",
        "            \"collection_name\": self.collection_name,\n",
        "            \"document_count\": self.collection.count(),\n",
        "            \"persist_directory\": self.persist_directory\n",
        "        }\n",
        "\n",
        "# Initialize retriever\n",
        "retriever = NewsRetriever(\n",
        "    collection_name=\"rpp_news\",\n",
        "    persist_directory=\"../data/chromadb\"\n",
        ")\n",
        "\n",
        "# Add documents to collection\n",
        "retriever.add_documents(embedded_articles)\n",
        "\n",
        "# Get collection statistics\n",
        "stats = retriever.get_collection_stats()\n",
        "print(f\"\\nüìä Collection Statistics:\")\n",
        "print(f\"Collection name: {stats['collection_name']}\")\n",
        "print(f\"Document count: {stats['document_count']}\")\n",
        "print(f\"Persist directory: {stats['persist_directory']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query 1: \"√öltimas noticias de econom√≠a\"\n",
        "query1 = \"√öltimas noticias de econom√≠a\"\n",
        "results_df1 = retriever.query_to_dataframe(query1, n_results=5, embedder=embedder)\n",
        "\n",
        "print(f\"\\nüîç Query: '{query1}'\")\n",
        "print(f\"\\nüìã Top 5 Results:\")\n",
        "print(\"=\"*100)\n",
        "display(results_df1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query 2: \"Noticias sobre pol√≠tica\"\n",
        "query2 = \"Noticias sobre pol√≠tica\"\n",
        "results_df2 = retriever.query_to_dataframe(query2, n_results=5, embedder=embedder)\n",
        "\n",
        "print(f\"\\nüîç Query: '{query2}'\")\n",
        "print(f\"\\nüìã Top 5 Results:\")\n",
        "print(\"=\"*100)\n",
        "display(results_df2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query 3: \"Deportes y f√∫tbol\"\n",
        "query3 = \"Deportes y f√∫tbol\"\n",
        "results_df3 = retriever.query_to_dataframe(query3, n_results=5, embedder=embedder)\n",
        "\n",
        "print(f\"\\nüîç Query: '{query3}'\")\n",
        "print(f\"\\nüìã Top 5 Results:\")\n",
        "print(\"=\"*100)\n",
        "display(results_df3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. LangChain Pipeline Integration\n",
        "\n",
        "### Step 5Ô∏è‚É£: Orchestrate with LangChain\n",
        "\n",
        "End-to-end pipeline: Load RSS ‚Üí Tokenize ‚Üí Embed ‚Üí Store ‚Üí Retrieve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LangChain Pipeline Class\n",
        "class NewsRetrievalPipeline:\n",
        "    \"\"\"LangChain-based pipeline for news retrieval\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                 persist_directory: str = \"../data/langchain_chromadb\"):\n",
        "        print(\"Initializing LangChain NewsRetrievalPipeline...\")\n",
        "        \n",
        "        self.model_name = model_name\n",
        "        self.persist_directory = persist_directory\n",
        "        \n",
        "        print(f\"Loading embeddings model: {model_name}\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': 'cpu'},\n",
        "            encode_kwargs={'normalize_embeddings': True}\n",
        "        )\n",
        "        \n",
        "        self.text_splitter = CharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50,\n",
        "            separator=\"\\n\"\n",
        "        )\n",
        "        \n",
        "        self.vectorstore = None\n",
        "        print(\"Pipeline initialized successfully\")\n",
        "    \n",
        "    def load_articles_as_documents(self, articles: List[Dict]) -> List[Document]:\n",
        "        \"\"\"Convert articles to LangChain Document objects\"\"\"\n",
        "        print(f\"Converting {len(articles)} articles to LangChain Documents...\")\n",
        "        \n",
        "        documents = []\n",
        "        for article in articles:\n",
        "            page_content = f\"{article.get('title', '')}\\n{article.get('description', '')}\"\n",
        "            \n",
        "            metadata = {\n",
        "                \"title\": article.get(\"title\", \"\"),\n",
        "                \"description\": article.get(\"description\", \"\"),\n",
        "                \"link\": article.get(\"link\", \"\"),\n",
        "                \"published\": article.get(\"published\", \"\")\n",
        "            }\n",
        "            \n",
        "            doc = Document(page_content=page_content, metadata=metadata)\n",
        "            documents.append(doc)\n",
        "        \n",
        "        print(f\"Created {len(documents)} documents\")\n",
        "        return documents\n",
        "    \n",
        "    def create_vectorstore(self, documents: List[Document]) -> Chroma:\n",
        "        \"\"\"Create or update Chroma vector store with documents\"\"\"\n",
        "        print(f\"Creating vector store with {len(documents)} documents...\")\n",
        "        \n",
        "        self.vectorstore = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "        \n",
        "        print(f\"Vector store created and persisted to: {self.persist_directory}\")\n",
        "        return self.vectorstore\n",
        "    \n",
        "    def query(self, query_text: str, k: int = 5) -> List[Document]:\n",
        "        \"\"\"Query the vector store\"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            raise ValueError(\"Vector store not initialized. Call create_vectorstore first.\")\n",
        "        \n",
        "        print(f\"\\nQuerying: '{query_text}'\")\n",
        "        results = self.vectorstore.similarity_search(query_text, k=k)\n",
        "        print(f\"Found {len(results)} results\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def query_to_dataframe(self, query_text: str, k: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"Query and return results as pandas DataFrame\"\"\"\n",
        "        results = self.query(query_text, k)\n",
        "        \n",
        "        data = []\n",
        "        for doc in results:\n",
        "            row = {\n",
        "                \"title\": doc.metadata.get(\"title\", \"\"),\n",
        "                \"description\": doc.metadata.get(\"description\", \"\"),\n",
        "                \"link\": doc.metadata.get(\"link\", \"\"),\n",
        "                \"date_published\": doc.metadata.get(\"published\", \"\")\n",
        "            }\n",
        "            data.append(row)\n",
        "        \n",
        "        return pd.DataFrame(data)\n",
        "    \n",
        "    def run_pipeline(self, articles: List[Dict], query_text: str, k: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"Run the complete pipeline: load ‚Üí embed ‚Üí store ‚Üí query\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"RUNNING COMPLETE LANGCHAIN PIPELINE\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        print(\"\\n[Step 1/4] Loading articles as documents...\")\n",
        "        documents = self.load_articles_as_documents(articles)\n",
        "        \n",
        "        print(\"\\n[Step 2/4] Creating vector store with embeddings...\")\n",
        "        self.create_vectorstore(documents)\n",
        "        \n",
        "        print(\"\\n[Step 3/4] Querying vector store...\")\n",
        "        \n",
        "        print(\"\\n[Step 4/4] Formatting results...\")\n",
        "        df = self.query_to_dataframe(query_text, k)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        return df\n",
        "\n",
        "# Initialize LangChain pipeline\n",
        "pipeline = NewsRetrievalPipeline(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    persist_directory=\"../data/langchain_chromadb\"\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ LangChain pipeline initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run complete pipeline\n",
        "query_langchain = \"√öltimas noticias de econom√≠a\"\n",
        "\n",
        "results_langchain = pipeline.run_pipeline(\n",
        "    articles=articles,\n",
        "    query_text=query_langchain,\n",
        "    k=5\n",
        ")\n",
        "\n",
        "print(f\"\\nüîç LangChain Query: '{query_langchain}'\")\n",
        "print(f\"\\nüìã Top 5 Results from LangChain Pipeline:\")\n",
        "print(\"=\"*100)\n",
        "display(results_langchain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save query results to outputs folder\n",
        "output_dir = \"/outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save results\n",
        "results_df1.to_csv(f\"{output_dir}/query_economia.csv\", index=False, encoding='utf-8')\n",
        "results_df2.to_csv(f\"{output_dir}/query_politica.csv\", index=False, encoding='utf-8')\n",
        "results_df3.to_csv(f\"{output_dir}/query_deportes.csv\", index=False, encoding='utf-8')\n",
        "results_langchain.to_csv(f\"{output_dir}/query_langchain_economia.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "print(\"\\n‚úÖ Results saved to outputs folder:\")\n",
        "print(f\"   - {output_dir}/query_economia.csv\")\n",
        "print(f\"   - {output_dir}/query_politica.csv\")\n",
        "print(f\"   - {output_dir}/query_deportes.csv\")\n",
        "print(f\"   - {output_dir}/query_langchain_economia.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary & Deliverables\n",
        "\n",
        "### ‚úÖ Completed Tasks:\n",
        "\n",
        "1. **Step 0Ô∏è‚É£: Load Data** - Fetched 50 latest articles from RPP RSS feed\n",
        "2. **Step 1Ô∏è‚É£: Tokenization** - Analyzed token counts using tiktoken (cl100k_base)\n",
        "3. **Step 2Ô∏è‚É£: Embedding** - Generated embeddings using sentence-transformers/all-MiniLM-L6-v2\n",
        "4. **Step 3Ô∏è‚É£: ChromaDB Collection** - Created collection and stored documents with metadata\n",
        "5. **Step 4Ô∏è‚É£: Query Results** - Performed similarity search and displayed results in DataFrame\n",
        "6. **Step 5Ô∏è‚É£: LangChain Orchestration** - Implemented end-to-end pipeline\n",
        "\n",
        "### üìä Key Findings:\n",
        "\n",
        "- Successfully retrieved 50 articles from RPP RSS feed\n",
        "- Average tokens per article analyzed\n",
        "- Embeddings generated with 384 dimensions (all-MiniLM-L6-v2)\n",
        "- ChromaDB collection created with cosine similarity\n",
        "- Semantic search working correctly\n",
        "- LangChain pipeline fully functional\n",
        "\n",
        "### üìÅ Outputs:\n",
        "\n",
        "- Query results saved as CSV files\n",
        "- ChromaDB persisted for future use\n",
        "- All paths relative for reproducibility\n",
        "\n",
        "**Note**: All code is self-contained in this notebook.\n",
        "\n",
        "---\n",
        "\n",
        "**End of Notebook**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
